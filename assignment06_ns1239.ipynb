{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caffeinated-beverage/NikkiSingh_DTSC3020_Fall2025/blob/main/assignment06_ns1239.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_de5Eq4u-tR"
      },
      "source": [
        "# Assignment 6 (4 points) — Web Scraping\n",
        "\n",
        "In this assignment you will complete **two questions**. The **deadline is posted on Canvas**.\n"
      ],
      "id": "H_de5Eq4u-tR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PHwamZMu-tX"
      },
      "source": [
        "## Assignment Guide (Read Me First)\n",
        "\n",
        "- This notebook provides an **Install Required Libraries** cell and a **Common Imports & Polite Headers** cell. Run them first.\n",
        "- Each question includes a **skeleton**. The skeleton is **not** a solution; it is a lightweight scaffold you may reuse.\n",
        "- Under each skeleton you will find a **“Write your answer here”** code cell. Implement your scraping, cleaning, and saving logic there.\n",
        "- When your code is complete, run the **Runner** cell to print a Top‑15 preview and save the CSV.\n",
        "- Expected outputs:\n",
        "  - **Q1:** `data_q1.csv` + Top‑15 sorted by the specified numeric column.\n",
        "  - **Q2:** `data_q2.csv` + Top‑15 sorted by `points`.\n"
      ],
      "id": "4PHwamZMu-tX"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "I7DLq9nEu-tZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb859802-4182-4a33-8b74-cc195c8f1142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 4, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 11, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n",
            "    from pip._internal.build_env import get_runnable_pip\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/build_env.py\", line 19, in <module>\n",
            "    from pip._internal.cli.spinners import open_spinner\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n",
            "    from pip._internal.utils.logging import get_indentation\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/logging.py\", line 13, in <module>\n",
            "    from pip._vendor.rich.console import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/console.py\", line 47, in <module>\n",
            "    from . import errors, themes\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/themes.py\", line 2, in <module>\n",
            "    from .theme import Theme\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/theme.py\", line 1, in <module>\n",
            "    import configparser\n",
            "  File \"/usr/lib/python3.12/configparser.py\", line 541, in <module>\n",
            "    class RawConfigParser(MutableMapping):\n",
            "  File \"<frozen abc>\", line 106, in __new__\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Dependencies installed.\n"
          ]
        }
      ],
      "source": [
        "#1) Install Required Libraries\n",
        "!pip -q install requests beautifulsoup4 lxml pandas\n",
        "print(\"Dependencies installed.\")\n"
      ],
      "id": "I7DLq9nEu-tZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug_A9RuPu-tb"
      },
      "source": [
        "### 2) Common Imports & Polite Headers"
      ],
      "id": "ug_A9RuPu-tb"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ov8pXh65u-tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "779917f0-e170-44b4-9dcd-46566eeb9246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common helpers loaded.\n"
          ]
        }
      ],
      "source": [
        "# Common Imports & Polite Headers\n",
        "import re, sys, pandas as pd, requests\n",
        "from bs4 import BeautifulSoup\n",
        "HEADERS = {\"User-Agent\": (\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "    \"(KHTML, like Gecko) Chrome/122.0 Safari/537.36\")}\n",
        "def fetch_html(url: str, timeout: int = 20) -> str:\n",
        "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "def flatten_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [\" \".join([str(x) for x in tup if str(x)!=\"nan\"]).strip()\n",
        "                      for tup in df.columns.values]\n",
        "    else:\n",
        "        df.columns = [str(c).strip() for c in df.columns]\n",
        "    return df\n",
        "print(\"Common helpers loaded.\")\n"
      ],
      "id": "Ov8pXh65u-tc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km0GO7zzu-td"
      },
      "source": [
        "## Question 1 — IBAN Country Codes (table)\n",
        "**URL:** https://www.iban.com/country-codes  \n",
        "**Extract at least:** `Country`, `Alpha-2`, `Alpha-3`, `Numeric` (≥4 cols; you may add more)  \n",
        "**Clean:** trim spaces; `Alpha-2/Alpha-3` → **UPPERCASE**; `Numeric` → **int** (nullable OK)  \n",
        "**Output:** write **`data_q1.csv`** and **print a Top-15** sorted by `Numeric` (desc, no charts)  \n",
        "**Deliverables:** notebook + `data_q1.csv` + short `README.md` (URL, steps, 1 limitation)\n",
        "\n",
        "**Tip:** You can use `pandas.read_html(html)` to read tables and then pick one with ≥3 columns.\n"
      ],
      "id": "km0GO7zzu-td"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "q1_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q1 Skeleton (fill the TODOs) ---\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Return the first table with >= 3 columns from the HTML.\n",
        "    Uses pd.read_html and returns the first table it finds.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tables = pd.read_html(StringIO(html))\n",
        "    except ImportError:\n",
        "        print(\"Error: `lxml` library not found. Please install it with 'pip install lxml'\", file=sys.stderr)\n",
        "        return pd.DataFrame()\n",
        "    except ValueError:\n",
        "        print(\"Error: No tables were found in the provided HTML.\", file=sys.stderr)\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    for df in tables:\n",
        "        if df.shape[1] >= 3:\n",
        "            return df.copy()\n",
        "\n",
        "    raise ValueError(\"No table with >= 3 columns was found in the HTML.\")\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Clean columns: strip, UPPER Alpha-2/Alpha-3, cast Numeric to int (nullable), drop invalids.\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    rename_map = {\n",
        "        'Country': 'Country',\n",
        "        'Alpha-2 code': 'Alpha-2',\n",
        "        'Alpha-3 code': 'Alpha-3',\n",
        "        'Numeric': 'Numeric'\n",
        "    }\n",
        "    df_clean = df_clean.rename(columns=rename_map)\n",
        "\n",
        "    required_cols = ['Country', 'Alpha-2', 'Alpha-3', 'Numeric']\n",
        "\n",
        "    missing_cols = [col for col in required_cols if col not in df_clean.columns]\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Missing required columns after rename: {missing_cols}\")\n",
        "\n",
        "    df_clean = df_clean[required_cols]\n",
        "\n",
        "    df_clean['Country'] = df_clean['Country'].str.strip()\n",
        "    df_clean['Alpha-2'] = df_clean['Alpha-2'].str.strip().str.upper()\n",
        "    df_clean['Alpha-3'] = df_clean['Alpha-3'].str.strip().str.upper()\n",
        "\n",
        "    df_clean['Numeric'] = pd.to_numeric(df_clean['Numeric'], errors='coerce')\n",
        "\n",
        "    df_clean = df_clean.dropna(subset=['Numeric'])\n",
        "\n",
        "    df_clean['Numeric'] = df_clean['Numeric'].astype(pd.Int64Dtype())\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Sort descending by Numeric and return Top-N.\n",
        "    \"\"\"\n",
        "    df_sorted = df.sort_values(by='Numeric', ascending=False)\n",
        "\n",
        "    return df_sorted.head(top)\n"
      ],
      "id": "q1_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "q1_skeleton_answer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d85327e-386b-485f-9d57-2d5ee1ee17ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data from https://www.iban.com/country-codes...\n",
            "Parsing HTML table...\n",
            "Cleaning data...\n",
            "Sorting and getting top 15...\n",
            "\n",
            "--- Top 15 Countries by Numeric Code (Descending) ---\n",
            "                                                   Country Alpha-2 Alpha-3  Numeric\n",
            "                                                    Zambia      ZM     ZMB      894\n",
            "                                                     Yemen      YE     YEM      887\n",
            "                                                     Samoa      WS     WSM      882\n",
            "                                         Wallis and Futuna      WF     WLF      876\n",
            "                        Venezuela (Bolivarian Republic of)      VE     VEN      862\n",
            "                                                Uzbekistan      UZ     UZB      860\n",
            "                                                   Uruguay      UY     URY      858\n",
            "                                              Burkina Faso      BF     BFA      854\n",
            "                                     Virgin Islands (U.S.)      VI     VIR      850\n",
            "                            United States of America (the)      US     USA      840\n",
            "                              Tanzania, United Republic of      TZ     TZA      834\n",
            "                                               Isle of Man      IM     IMN      833\n",
            "                                                    Jersey      JE     JEY      832\n",
            "                                                  Guernsey      GG     GGY      831\n",
            "United Kingdom of Great Britain and Northern Ireland (the)      GB     GBR      826\n",
            "\n",
            "Successfully saved all 249 cleaned rows to data_q1.csv\n"
          ]
        }
      ],
      "source": [
        "# Q1 — Write your answer here\n",
        "import requests\n",
        "import pandas as pd\n",
        "import sys\n",
        "from io import StringIO\n",
        "\n",
        "def main():\n",
        "    URL = \"https://www.iban.com/country-codes\"\n",
        "    HEADERS = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    print(f\"Fetching data from {URL}...\")\n",
        "    try:\n",
        "        response = requests.get(URL, headers=HEADERS)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        print(\"Parsing HTML table...\")\n",
        "        df_raw = q1_read_table(response.text)\n",
        "\n",
        "        print(\"Cleaning data...\")\n",
        "        df_clean = q1_clean(df_raw)\n",
        "\n",
        "        print(\"Sorting and getting top 15...\")\n",
        "        df_top15 = q1_sort_top(df_clean, top=15)\n",
        "\n",
        "        print(\"\\n--- Top 15 Countries by Numeric Code (Descending) ---\")\n",
        "        print(df_top15.to_string(index=False))\n",
        "\n",
        "        output_filename = \"data_q1.csv\"\n",
        "        df_clean.to_csv(output_filename, index=False, encoding='utf-8')\n",
        "        print(f\"\\nSuccessfully saved all {len(df_clean)} cleaned rows to {output_filename}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during HTTP request: {e}\", file=sys.stderr)\n",
        "    except (ValueError, KeyError) as e:\n",
        "        print(f\"Error during data processing: {e}\", file=sys.stderr)\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\", file=sys.stderr)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "id": "q1_skeleton_answer"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmefu--_u-tg"
      },
      "source": [
        "## Question 2 — Hacker News (front page)\n",
        "**URL:** https://news.ycombinator.com/  \n",
        "**Extract at least:** `rank`, `title`, `link`, `points`, `comments` (user optional)  \n",
        "**Clean:** cast `points`/`comments`/`rank` → **int** (non-digits → 0), fill missing text fields  \n",
        "**Output:** write **`data_q2.csv`** and **print a Top-15** sorted by `points` (desc, no charts)  \n",
        "**Tip:** Each story is a `.athing` row; details (points/comments/user) are in the next `<tr>` with `.subtext`.\n"
      ],
      "id": "rmefu--_u-tg"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "q2_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q2 Skeleton (fill the TODOs) ---\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Parse front page items into DataFrame columns:\n",
        "        rank, title, link, points, comments, user.\n",
        "    Uses BeautifulSoup to find '.athing' rows and their\n",
        "    following '.subtext' sibling rows.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, 'lxml')\n",
        "    stories = []\n",
        "\n",
        "    item_rows = soup.find_all('tr', class_='athing')\n",
        "\n",
        "    for item_row in item_rows:\n",
        "        rank_tag = item_row.find('span', class_='rank')\n",
        "        title_span = item_row.find('span', class_='titleline')\n",
        "        title_link = title_span.find('a') if title_span else None\n",
        "\n",
        "        subtext_td = None\n",
        "        next_tr = item_row.find_next_sibling('tr')\n",
        "        if next_tr:\n",
        "            subtext_td = next_tr.find('td', class_='subtext')\n",
        "\n",
        "        points_tag = None\n",
        "        user_tag = None\n",
        "        comments_text = None\n",
        "\n",
        "        if subtext_td:\n",
        "            points_tag = subtext_td.find('span', class_='score')\n",
        "\n",
        "            user_tag = subtext_td.find('a', class_='hnuser')\n",
        "\n",
        "            comment_links = subtext_td.find_all('a')\n",
        "            if comment_links:\n",
        "                last_link = comment_links[-1]\n",
        "                if 'comment' in last_link.text or 'discuss' in last_link.text:\n",
        "                    comments_text = last_link.text\n",
        "\n",
        "        story_data = {\n",
        "            'rank': rank_tag.text if rank_tag else None,\n",
        "            'title': title_link.text if title_link else None,\n",
        "            'link': title_link.get('href') if title_link else None,\n",
        "            'points': points_tag.text if points_tag else None,\n",
        "            'user': user_tag.text if user_tag else None,\n",
        "            'comments': comments_text\n",
        "        }\n",
        "        stories.append(story_data)\n",
        "\n",
        "    return pd.DataFrame(stories)\n",
        "\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Clean numeric fields and fill missing values.\n",
        "    Casts points/comments/rank to int (non-digits -> 0).\n",
        "    Fills missing text fields with empty string.\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    df_clean['rank'] = df_clean['rank'].astype(str).str.extract(r'(\\d+)').fillna(0).astype(int)\n",
        "    df_clean['points'] = df_clean['points'].astype(str).str.extract(r'(\\d+)').fillna(0).astype(int)\n",
        "    df_clean['comments'] = df_clean['comments'].astype(str).str.extract(r'(\\d+)').fillna(0).astype(int)\n",
        "\n",
        "    df_clean['title'] = df_clean['title'].fillna('')\n",
        "    df_clean['link'] = df_clean['link'].fillna('')\n",
        "    df_clean['user'] = df_clean['user'].fillna('')\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points desc and return Top-N.\"\"\"\n",
        "    df_sorted = df.sort_values(by='points', ascending=False)\n",
        "    return df_sorted.head(top)"
      ],
      "id": "q2_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "q2_skeleton_answer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86107720-9e6a-4396-a85d-0e35ee52337c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching data from https://news.ycombinator.com/...\n",
            "Parsing HTML...\n",
            "Cleaning data...\n",
            "Sorting and getting top 15...\n",
            "\n",
            "--- Top 15 Hacker News Posts by Points (Descending) ---\n",
            " rank  points  comments                                                                           title\n",
            "    1     536       265                                                Solarpunk is happening in Africa\n",
            "   28     326       178 Norway reviews cybersecurity after remote-access feature found in Chinese buses\n",
            "    6     322       142             New gel restores dental enamel and could revolutionise tooth repair\n",
            "   20     284       176                I was right about dishwasher pods and now I can prove it [video]\n",
            "   13     252        81                                            The shadows lurking in the equations\n",
            "   24     232       108  SPy: An interpreter and compiler for a fast statically typed variant of Python\n",
            "    2     229        80                                   Dillo, a multi-platform graphical web browser\n",
            "   14     205        67                                  An eBPF Loophole: Using XDP for Egress Traffic\n",
            "   15     204       162                                     NY smartphone ban has made lunch loud again\n",
            "    3     203       208  ChatGPT terms disallow its use in providing legal and medical advice to others\n",
            "    8     183       309                                                Why aren't smart people happier?\n",
            "   22     182       139                                         Carice TC2 – A non-digital electric car\n",
            "   23     177        38                                    Apple App Store frontend source code archive\n",
            "    9     167        98                                               Ruby and Its Neighbors: Smalltalk\n",
            "   17     165        46   Vacuum bricked after user blocks data collection – user mods it to run anyway\n",
            "\n",
            "Successfully saved all 30 posts to data_q2.csv\n"
          ]
        }
      ],
      "source": [
        "# Q2 — Write your answer here\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import sys\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def main():\n",
        "    URL = \"https://news.ycombinator.com/\"\n",
        "    HEADERS = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    print(f\"Fetching data from {URL}...\")\n",
        "    try:\n",
        "        response = requests.get(URL, headers=HEADERS)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        print(\"Parsing HTML...\")\n",
        "        df_raw = q2_parse_items(response.text)\n",
        "\n",
        "        print(\"Cleaning data...\")\n",
        "        df_clean = q2_clean(df_raw)\n",
        "\n",
        "        print(\"Sorting and getting top 15...\")\n",
        "        df_top15 = q2_sort_top(df_clean, top=15)\n",
        "\n",
        "        print(\"\\n--- Top 15 Hacker News Posts by Points (Descending) ---\")\n",
        "        display_cols = ['rank', 'points', 'comments', 'title']\n",
        "        print(df_top15[display_cols].to_string(index=False))\n",
        "\n",
        "        output_filename = \"data_q2.csv\"\n",
        "        df_clean.to_csv(output_filename, index=False, encoding='utf-8')\n",
        "        print(f\"\\nSuccessfully saved all {len(df_clean)} posts to {output_filename}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during HTTP request: {e}\", file=sys.stderr)\n",
        "    except (ValueError, KeyError, AttributeError) as e:\n",
        "        print(f\"Error during data processing: {e}\", file=sys.stderr)\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\", file=sys.stderr)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "id": "q2_skeleton_answer"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}